\chapter{Literature Review}
Our work draws on a significant amount of previous research. In this chapter, we will critically reflect on and discuss that work and its impact on the social media research field.
\section{Information Trustworthiness Online}
In this section we discuss the state-of-the-art in this field and seek to understand what information trustworthiness is online. Information Trustworthiness, in the context of online interactions is content that is reliable and credible. Trustworthiness online could be assessed by any combination of factors including User-based Features, Message-based features, Topic-Based features and Propagation-Based features\cite{12}. Our goal here is to identify the different types of factors and features explored and their impact on trustworthiness, and narrow it down to what would be most relevant for our particular case, i.e, Twitter. In this document, by trust $\mathbf{Factors}$ we mean factors such as Corroboration, Competence of the Source, Propagation, Information Provenance etc. Trust \textbf{Features} would be domain specific and closely related to the problem. For example, the number of likes of a Facebook page, or the number of friends of a Facebook user would all be referred to as features.
\subsection{Non-Twitter Based Research} 
A paper by Yahoo! Research\cite{5}, addresses the problem of finding high quality content in community-driven question/answering sites. Quality of content might not always correlate with its trustworthiness. Research and Literature though, has highlighted that they certainly share similar characteristics. For this research, they looked specifically at Intrinsic content quality (Punctuation, typos, syntactics and semantic complexity, grammaticality), User Relationships (User \textit{a} answered a question by user \textit{b}) and Usage statistics (number of clicks on a question). One of the primary differences of this research to our problem is that the actors involved are different. In the question and answer domain, there is an asker, answerer and an evaluator\footnote{People who up vote answers, like answers, modify them or question them}. In the case of Twitter there is only a User and a `Fan'. We use the word fan to describe any other Twitter user who has favourited or retweeted the tweet. In terms of the domain, question and answer platforms are used by users looking for help with a particular situation\cite{5}, however broadcasting platforms such as Twitter are used as ways of displaying ones thoughts, current events or generally making one's voice heard. ``Size Matters" by Blumenstock\cite{4} proposes word count as a factor for determining article quality. While this might stand true for articles on Wikipedia, the platform on which they base their research, this may not necessarily make a huge difference in the results we get since a tweet is a maximum of 140 characters regardless. Smyth et al. \cite{10} state that features such as regularity with which bloggers post, timeliness of the posts, post length, spelling quality and appropriate use of capitalisation and emoticons in the text link to an article's credibility, a factor which has close links to Trustworthiness. They specifically look at Amazon Reviews and TripAdvisor for their test cases.  In Nurse et a. \cite{7}, they identify timeliness as one of the main factors in trustworthiness. While this may be true in crisis situations or in relation to current affairs, this may not necessarily apply to general tweets, or tweets about past events. They also state that information completeness, complexity and relevance are important factors in assessing trustworthiness. Besides they've identified a fair few factors such as authority/reputation affecting trustworthiness, which we have factored into our model. Goldsmith et al.\cite{13} provides a state of the art review of the current work and considers the links between provenance, quality and trustworthiness. They provide a number of factors which undoubtedly would have an influence on our analysis.
\subsection{Twitter Based Research}
Gupta et al. \cite{11} is quite closely related to our research. However, the address tweets of high impact events as opposed to any general tweet. In it they identify no. of characters, emoticons present in a tweet, number of followers, length of username and use of pronouns and swear words as some of the more important content-based credibility features. Carlos et al. \cite{12} look at analysing microblog postings from trending topics and aim to classify them as credible or not credible. They categorise factors into four different sections. They are Message-based, User-based, Topic-based and Propagation-based factors. They identify a range of features, such as length of words, length of characters, contains a question mark, distinct hashtags etc. Morris et al. \cite{17}, examine key elements of the information interface for their impact on humans' credibility judgements. They show that users had difficulty determining the truthfulness of content and that their judgements were based on heuristics such as retweets and topic-related user names were considered more trustworthy than another. For example, the user name `TechCrunch' for a Twitter profile would appear more trustful to humans than a name such as `Jo' if they were to believe news about technology. Researchers at the University of Washington \cite{43}, are currently working on looking at the relationship between various web links within tweets, and the quality of information spread during the Boston marathon bombing incident\footnote{\url{http://en.wikipedia.org/wiki/Boston_Marathon_bombings}}. They are hoping to develop a tool, which would let users know when a tweet is being questioned or proven untrue by another tweet. \\*\\*
We discussed the different types of research both Twitter based and non-Twitter based conducted in this domain, in the above section. The following section will detail and explain some of the metrics, models and methodologies used in the past to predict quality, credibility or trustworthiness of tweets. 
\section{Measuring Trustworthiness}
\subsection{Metrics and Models}
Here we focus on the types of features/characteristics that researchers have chosen in the data to analyse, as well as the techniques that have been used.
Nurse et al. \cite{7}, reflects on information trust and quality metrics. They propose some new metrics that are worthy of consideration. This acted as a good source to think about different metrics for trust on social media and build our research.
In our research we are not necessarily looking at content quality hence we do not have these characteristics factored into our model. In the Yahoo! research\cite{5}, they selected the 20 most significant features for question quality using a chi-squared test and discovered that the precision and recall improved slightly if they used the top 10 features as opposed to all the features. The precision, recall and Area under curve\footnote{A graph of True positives Vs False positives. See Chapter 6 for explanations} for finding high quality answers was nearly 90\% in comparison to the 70\% obtained for question quality. This probably could be due to the fact that they used the the text as a baseline and the answer text would have more content to it than the question hence a more accurate prediction in the latter case. \\*\\*For the experimentation in \cite{7} they used the following factors as basis : Corroboration (extent to which the same information originates from different sources), Social-Jargon (e.g Emoticons, Shouting), Competence of the Source (Level of expertise of the source of information), Location of the source. In ``Information Credibility on Twitter"\cite{12} they used Machine Learning techniques such as the Support Vector Machines (see 5.3), decision trees (see 5.4.1), decision rules and Bayes networks but achieved best results using a J48 decision tree method. \\*\\*
The article about Wikipedia \cite{4}, which states how word count affects quality, achieved above a 95\% accuracy on Machine Learning models such as multi-layer perceptron, k-nearest neighbour classifier, a logit model and a random forest classifier. Kumaraguru et al. \cite{11}, state that they found that on average 30\% content about an event provides situational awareness information and 14\% was spam. They also found that only 17\% of the situational awareness information was credible\cite{11},. They mention that one of the drawbacks and limitations with their data collection method was that they had to allow human annotators to annotate the classifications and would like a more automated system to establish ground truth. Our proposed algorithm seeks to tackle this problem by learning from known Trustworthy and Untrustworthy. 
\subsection{Data Collection and Classification Methodology}
We discuss how different researchers approach the problem of classifying data into different classes. Our approach to classification for this problem was exploring news agency tweets as annotated Trustworthy content and fake/known parody accounts as Untrustworthy content. However it is interesting to see how other researchers have handled the data classification problem so we detail some classification methods in this section. The dataset used by Yahoo! Research consisted of 8,366 questions/answer pairs and 6665 questions. There were independent human editors who labelled all of the above data for quality. One would find that a drawback with such a method for labelling data could be that it might be slightly biased based the ranker's knowledge / previous experience with the topic at hand. TThe paper ``Tweeting is Believing"\cite{17} states that they find that users are poor judges of truthfulness based on content alone. Our approach tries to be different as we do not involve human annotators at the Classification stage, although we appreciate that it is an assumption that all news tweets are Trustworthy. For the study on Amazon and TripAdvisor they collected four large review datasets and they relied on the response provided by the user in whether they found the reviews to be helpful or not to establish ground truth. In the paper about credibility of tweets \cite{11} they use the Twitter Streaming API to search for keywords and select tweets based on that. They also used the Trends API which returned the trending current trending topics in Twitter. Human annotators were used to rate the credibility of the tweets into four different categories such as Definitely Credible, Seems Credible, Definitely Not Credible, I cannot Decide. The paper by Castillo \cite{12} looks at time sensitive information such as current news events and hence they used the Twitter Monitor\footnote{http://twittermonitor.net} which monitors sharp increases in the frequency in a set of keywords in a tweets. For classifying they used the Mechanical Turk\footnote{http://www.mturk.com} which involved asking evaluators to assist them with classification. 
\section{Summary}
This review enabled us to understand factors and features, metrics and models and how they are all connected to each other. We also learned about the ongoing research in this field in both academia and in the industry. We realise that the problem of determining Trustworthiness of content still exists and we were able to identify three key areas in terms of factor selection that we would focus our research on.  We chose Message content-based factors, User-based factors and Tweet-based factors, the reasons behind this selection will be explained in future chapters. The next chapter explains in detail our approach to the problem and how we compartmentalised the  necessary steps to take to achieve our aim.  




