\chapter{Literature Review}
Our work draws on a significant amount of research done prior to this and in this chapter we will outline and discuss some of the related work done before we introduce our model in the next chapter. 
\\*\\*
The research problem we address is identifying trustworthy content amongst data in social media. An extensive literature review was done on this topic to identify very specific articles and general ideas that could be gathered from related topics. 
\section{Research Types, Domains, Similarity}
Let us first look at the different types of related research done and the domains they were carried out on and see how similar or different they are to our problem. "Finding High Quality Content in Social Media"[5] a paper by Yahoo! Research, addresses the problem of finding high quality content in community-driven question/answering sites. They focus mainly on the on question/answering domain and have achieved an accuracy close to that of humans. Quality of content doesn't necessarily relate to trustworthiness of content but from our analysis we find that they certainly share similar characteristics. One of the primary differences of this research to our problem is that the actors involved are different. Here you have an asker, answerer and an evaluator. We have just a User and a fan. For the lack of a better word, we use the word fan to describe anyone who would favourite or retweet the tweet. In terms of the domain, question and answer platforms are used by users looking for help with a particular situation[5], however broadcasting platforms such as ours are used as ways of displaying ones thoughts, current events or generally making your voice heard. The paper "Supporting Human Decision-Making Online Using Information-Trustworthiness Metrics"[7] reflects information trust and quality metrics. They propose some new metrics that are worthy of consideration. This acted as a good read to think about different metrics for trust on social media. The Article "Size Matters" by Joshua Blumenstock[9] proposes word count as a metric for measuring article quality. While this stands true for articles on Wikipedia, on which they base their research, this may not necessarily make a huge difference in the results we get since a tweet is 140 characters regardless. "Using Readability Tests to Predict Helpful Product Reviews"[10] states that credibility features such as regularity with which bloggers post, timeliness of the posts, post length, spelling quality and appropriate use of capitalisation and emoticons in the text were found to improve performance. They specifically look at Amazon Reviews and TripAdvisor. "Credibility Ranking of Tweets during High Impact Events"[11] is quite closely related to our research, however they look at tweets of high impact events as opposed to any general tweet. They identify that characters, emoticons in a tweet, number of followers and length of username, pronouns and swear words as some of the more important content based features. The paper "Information Credibility on Twitter"[12] by Carlos Castillo looks at analysing microblog postings from trending topics and aims to classify them as credible or not credible. "Information Quality and Trustworthiness"[13] provides a state of the art review of the current work and considers the links between provenance, quality and trustworthiness. They provide a number of factors which we have used in our analysis and related these factors to Quality, Provenance or Trustworthiness. 
\section{Data Collection}
The dataset used by Yahoo! Research consisted of 8,366 questions/answer pairs and 6665 questions. There were independent human editors who labelled all of the above data for quality. One would find that a drawback with such a method for labelling your data could be that it could be slightly biased based the ranker's knowledge / previous experience with the topic at hand. The Data Collection method was not discussed on the paper on Information-Trustworthiness Metrics however, they did use the dataset from the London 2011 riots both tweets from the public and news reports. For the study on Amazon and TripAdvisor they collected four large review datasets. In the paper about credibility of tweets[11] they use the Twitter Streaming API to search for key words and select tweets based on that. They Also used the Trends API which returned the trending current trending topics in twitter. They also used human annotators to rate the credibility of the tweets into four different categories. The paper by Castillo[12] looks at time sensitive information such as current news events and hence they used the $Twitter Monitor^1$ which monitors sharp increases in the frequency in a set of keywords in a message. For Classifying they used the $Mechanical Truk^2$ and asked evaluators to assist them with classification. 
\section{Analysis and Techniques}
Here we focus on the types of features/characteristics chosen from the data to analyse as well as the techniques used. The experiment by the Yahoo! Research team, looked specifically at Intrinsic content quality(Punctuation, typos, syntactics and semantic complexity, grammaticality), User Relationships(User \textit{a} answered a question by user \textit{b}) and Usage statistics(number of clicks on something). In our research we are not necessarily looking at content quality hence we do not have these characteristics factored into our model. They also look at question quality and answer quality separately. The paper on Information-Trustworthiness metrics identifies timeliness as one of the main factors in trustworthiness. While this may be true in crisis situations or in relation to current affairs, this may not necessarily apply to general tweets, or tweets about past events. They also state that information completeness, complexity and relevance are important factors in assessing trustworthiness. They've also identified a fair few factors such as authority/reputation affecting trustworthiness, which we have factored into our model. For the experimentation they used the following factors as basis : Corroboration(extent to which the same information originates from different sources), Social-Jargon(e.g Emoticons, Shouting), Competence of the Source(Level of expertise of the source of information), Location of the source. In the paper "Information Credibility on Twitter" they separate the tweets into four types, and have Message-based features, User-based Features, Topic-based features and Propagation-based features. This is only slightly different to ours, that we don't have topic based features. They tried a couple of learning techniques such as the SVM, decision trees, decision rules and Bayes networks but achieved best results using a J48 decision tree method. 
\section{Results, Conclusions and Unfinished Work}
In the paper by Yahoo! research, they selected the 20 most significant features for question quality using a chi-squared test and discovered that the precision and recall improved slightly if they used the top 10 features as opposed to all the features. The precision, recall and Area under curve for finding high quality answers was nearly 90\% in comparison to the 70\% obtained for question quality. This probably could be due to the fact that they used the the text as a baseline and the answer text would have more content to it than the question hence a more accurate prediction in the latter case. The article about how word count affects quality, achieved above 95\% accuracy on a multi-layer perceptron, k-nearest neighbour classifier, a logit model and a random forest classifier. The paper on Credibility ranking for High Impact Events states that they found that on average 30\% content about an event provides situational awareness information and 14\% was spam. They also found that only 17\% of the situational awareness information was credible[11]. They mention that one of the drawbacks and limitations with their data collection method was that they had to allow human annotators to annotate the classifications and would like a more automated system to establish ground truth. Our algorithm, tackles this problem by learning from known good and bad tweets. 









Foot Note : 1 http://twittermonitor.net, 2 http://www.mturk.com